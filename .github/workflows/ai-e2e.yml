# Apply-Copilot AI-Driven E2E Testing
# Automated testing with Claude Code following TDD methodology

name: AI Real-World Tests

on:
  pull_request:
    branches: [ main, develop ]
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: true
        default: 'full'
        type: choice
        options:
        - unit
        - integration
        - e2e
        - full

env:
  NODE_VERSION: '20'
  PYTHON_VERSION: '3.11'
  PLAYWRIGHT_BROWSERS_PATH: ${{ github.workspace }}/ms-playwright

jobs:
  # Unit and Integration Tests
  test-foundation:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: apply_copilot_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: ðŸ”„ Checkout code
        uses: actions/checkout@v4
      
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ“¦ Install Python dependencies
        run: |
          pip install --upgrade pip
          cd apps/api && pip install -r requirements.txt
          cd ../companion && pip install -r requirements.txt
          pip install pytest-cov pytest-xdist
      
      - name: ðŸ”§ Setup test environment
        run: |
          cp .env.example .env.test
          echo "DATABASE_URL=postgresql://postgres:postgres@localhost:5432/apply_copilot_test" >> .env.test
          echo "REDIS_URL=redis://localhost:6379/0" >> .env.test
          echo "COMPANION_TOKEN=test-token-1234567890123456789012345678901234" >> .env.test
          echo "SECRET_KEY=test-secret-key-1234567890123456789012345678901234" >> .env.test
          echo "LLM_PROVIDER=none" >> .env.test
          echo "DEBUG=true" >> .env.test
          mv .env.test .env
      
      - name: ðŸ§ª Run unit tests
        run: |
          cd apps/api && python -m pytest --maxfail=1 -q -m "not integration and not e2e" --cov=app --cov-report=xml
          cd ../companion && python -m pytest --maxfail=1 -q -m "not integration and not e2e"
      
      - name: ðŸ”— Run integration tests
        run: |
          cd apps/api && python -m pytest --maxfail=1 -q -m "integration"
          cd ../companion && python -m pytest --maxfail=1 -q -m "integration"
      
      - name: ðŸ“Š Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./apps/api/coverage.xml
          flags: unittests
          name: codecov-umbrella
  
  # E2E Tests with Playwright
  test-e2e:
    runs-on: ubuntu-latest
    needs: test-foundation
    
    steps:
      - name: ðŸ”„ Checkout code
        uses: actions/checkout@v4
      
      - name: ðŸŸ¢ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ“¦ Install dependencies
        run: |
          npm install -g pnpm
          pnpm install
          cd apps/api && pip install -r requirements.txt
          cd ../companion && pip install -r requirements.txt
          cd ../e2e && pnpm install
      
      - name: ðŸŽ­ Install Playwright browsers
        run: |
          cd apps/e2e && npx playwright install --with-deps
      
      - name: ðŸ”§ Setup test environment
        run: |
          cp .env.example .env
          echo "DATABASE_URL=sqlite:///test.db" >> .env
          echo "COMPANION_TOKEN=test-token-1234567890123456789012345678901234" >> .env
          echo "SECRET_KEY=test-secret-key-1234567890123456789012345678901234" >> .env
          echo "LLM_PROVIDER=none" >> .env
          echo "DEBUG=true" >> .env
      
      - name: ðŸ—ï¸ Build applications
        run: |
          make build
      
      - name: ðŸš€ Start services
        run: |
          make up
          sleep 10
      
      - name: ðŸ§ª Run E2E tests
        run: |
          cd apps/e2e && npx playwright test --reporter=list
      
      - name: ðŸ“Š Upload E2E test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: playwright-results
          path: apps/e2e/test-results/
      
      - name: ðŸ“¸ Upload E2E screenshots
        uses: actions/upload-artifact@v3
        if: failure()
        with:
          name: playwright-screenshots
          path: apps/e2e/test-results/
      
      - name: ðŸ“‹ Upload E2E report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: playwright-report
          path: apps/e2e/playwright-report/

  # AI-Driven Testing with Claude Code
  ai-testing:
    runs-on: ubuntu-latest
    needs: [test-foundation, test-e2e]
    if: github.event_name == 'pull_request' || github.event.inputs.test_level == 'full'
    
    steps:
      - name: ðŸ”„ Checkout code
        uses: actions/checkout@v4
      
      - name: ðŸŸ¢ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ¤– Install Claude Code
        run: |
          npm install -g @anthropic-ai/claude-code
      
      - name: ðŸ“¦ Install dependencies
        run: |
          npm install -g pnpm
          make install
      
      - name: ðŸ”§ Setup AI test environment
        run: |
          cp .env.example .env
          echo "DATABASE_URL=sqlite:///ai_test.db" >> .env
          echo "COMPANION_TOKEN=ai-test-token-1234567890123456789012345678901234" >> .env
          echo "SECRET_KEY=ai-test-secret-key-1234567890123456789012345678901234" >> .env
          echo "LLM_PROVIDER=none" >> .env
          echo "DEBUG=true" >> .env
          echo "DISABLE_AUTH=true" >> .env  # Only for CI testing
      
      - name: ðŸ¤– Run AI-driven test automation
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          chmod +x bin/ai-test.sh
          timeout 1800 bash bin/ai-test.sh || {
            echo "âš ï¸ AI testing completed with issues or timeout"
            echo "Check logs for details"
            exit 1
          }
      
      - name: ðŸ“‹ Upload AI test logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: ai-test-logs
          path: ai-test-*.log
      
      - name: ðŸ“Š Create test summary
        if: always()
        run: |
          echo "## ðŸ¤– AI-Driven Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if grep -q "ALL GREEN" ai-test-*.log 2>/dev/null; then
            echo "âœ… **SUCCESS**: All tests passed with AI automation" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **ATTENTION**: AI automation needs review" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Coverage:" >> $GITHUB_STEP_SUMMARY
          echo "- Unit Tests: âœ…" >> $GITHUB_STEP_SUMMARY
          echo "- Integration Tests: âœ…" >> $GITHUB_STEP_SUMMARY  
          echo "- E2E Tests: âœ…" >> $GITHUB_STEP_SUMMARY
          echo "- AI Validation: $([ -f ai-test-*.log ] && echo 'âœ…' || echo 'âš ï¸')" >> $GITHUB_STEP_SUMMARY

  # Security and Quality Gate
  quality-gate:
    runs-on: ubuntu-latest
    needs: [test-foundation, test-e2e, ai-testing]
    if: always()
    
    steps:
      - name: ðŸ”„ Checkout code
        uses: actions/checkout@v4
      
      - name: ðŸ”’ Security scan
        run: |
          # Check for hard-coded secrets
          echo "Scanning for hard-coded secrets..."
          if grep -r "sk-[a-zA-Z0-9]" --include="*.py" --include="*.js" --include="*.ts" --exclude-dir=node_modules apps/ packages/ || \
             grep -r "nvapi-[a-zA-Z0-9]" --include="*.py" --include="*.js" --include="*.ts" --exclude-dir=node_modules apps/ packages/ || \
             grep -r "bearer.*[a-zA-Z0-9]" --include="*.py" --include="*.js" --include="*.ts" --exclude-dir=node_modules apps/ packages/; then
            echo "âŒ Hard-coded secrets detected!"
            exit 1
          fi
          echo "âœ… No hard-coded secrets found"
      
      - name: ðŸ“Š Quality gate summary
        run: |
          echo "## ðŸŽ¯ Quality Gate Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | ${{ needs.test-foundation.result == 'success' && 'âœ… Pass' || 'âŒ Fail' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.test-foundation.result == 'success' && 'âœ… Pass' || 'âŒ Fail' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| E2E Tests | ${{ needs.test-e2e.result == 'success' && 'âœ… Pass' || 'âŒ Fail' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| AI Validation | ${{ needs.ai-testing.result == 'success' && 'âœ… Pass' || 'âš ï¸ Review' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Security Scan | âœ… Pass |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.test-foundation.result }}" == "success" && "${{ needs.test-e2e.result }}" == "success" ]]; then
            echo "ðŸŽ‰ **Quality gate PASSED** - Ready for merge!" >> $GITHUB_STEP_SUMMARY
          else
            echo "ðŸš« **Quality gate FAILED** - Review required" >> $GITHUB_STEP_SUMMARY
          fi